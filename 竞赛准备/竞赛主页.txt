Title: CodaLab - Competition

URL Source: https://codalab.lisn.upsaclay.fr/competitions/21220

Markdown Content:
NTIRE Workshop and Challenges @ CVPR 2025
-----------------------------------------

Text to Image Generation Model Quality Assessment - Track 1 Image-Text Alignment
--------------------------------------------------------------------------------

### Important dates

*   **2025.01.10:** Release of train data (input and output) and validation data (inputs only)
*   **2025.01.30:** Validation server online
*   **2025.03.14:** Final test data release (inputs only)
*   **2025.03.21:** Test output results submission deadline
*   **2025.03.22:** Fact sheets and code/executable submission deadline
*   **2025.03.24:** Preliminary test results release to the participants
*   **2025.04.01:** Paper submission deadline for entries from the challenge
*   **2025.06.?:** NTIRE workshop and challenges, results and award ceremony (CVPR 2025, Nashville)

### Challenge overview

The 10th edition of NTIRE: New Trends in Image Restoration and Enhancement workshop will be held on June ?, 2025 in conjunction with CVPR 2025.

Image restoration, enhancement and manipulation are key computer vision tasks, aiming at the restoration of degraded image content, the filling in of missing information, or the needed transformation and/or manipulation to achieve a desired target (with respect to perceptual quality, contents, or performance of apps working on such images). Recent years have witnessed an increased interest from the vision and graphics communities in these fundamental topics of research. Not only has there been a constantly growing flow of related papers, but also substantial progress has been achieved.

Each step forward eases the use of images by people or computers for the fulfillment of further tasks, as image manipulation serves as an important frontend. Not surprisingly then, there is an ever-growing range of applications in fields such as surveillance, the automotive industry, electronics, remote sensing, or medical image analysis etc. The emergence and ubiquitous use of mobile and wearable devices offer another fertile ground for additional applications and faster methods.

This workshop aims to provide an overview of the new trends and advances in those areas. Moreover, it will offer an opportunity for academic and industrial attendees to interact and explore collaborations.

### Motivation

With the rise of Large Language Models (LLMs) and Diffusion Models, numerous text-to-image generation models have been proposed in recent years. The main goal of these generation models is to produce visual content based on user requirements, typically using prompts. The rapid advancements in text-to-image generation models have led to the development of numerous automated metrics for evaluating the alignment between generated images and their corresponding text descriptions. At the same time, the rapid development of text-to-image generation models has driven the creation of numerous automated metrics to evaluate the alignment between generated images and their corresponding text descriptions. However, the performance of these metrics is often limited by the size and granularity of existing datasets.

Jointly with the NTIRE workshop, we have an NTIRE challenge on image-text alignment evaluation. The task is to evaluate the alignment level between the given prompt and the generated image, and to determine whether the fine-grained elements in the text are reflected in the generated image. The aim is to develop a network design/solution that can generate image-text alignment scores more consistent with human preferences and achieve more accurate fine-grained alignment evaluation.

This track uses a new dataset called **EvalMuse-40K**, a comprehensive dataset consisting of approximately 40K meticulously annotated image-text pairs. This dataset is specifically designed to evaluate the alignment capabilities of text-to-image models at a fine-grained level.More details are found in the "get data" section of the competition, under the "participate" tab.

The top-ranked participants will be awarded and invited to follow the CVPR submission guide for workshops to describe their solutions and to submit to the associated NTIRE workshop at CVPR 2025.

**Provided Resources**

*   **Scripts:** With the dataset the organizers will provide scripts to facilitate the reproducibility of the images and performance evaluation results after the validation server is online. More information is provided on the data page.
*   **Contact:** You can use the forum on the data description page (highly recommended!) or directly contact the challenge organizers by email (Shuhao Han <hansh@mail.nankai.edu.cn\>, Haotian Fan <fanhaotian@bytedance.com\>, Chunle Guo <guochunle@nankai.edu.cn\> Chongyi Li <lichongyi@nankai.edu.cn\> Radu Timofte <radu.timofte@uni-wuerzburg.de\>) if you have doubts or any question.

NTIRE Workshop and Challenges @ CVPR 2025
-----------------------------------------

Text to Image Generation Model Quality Assessment - Track 1 Image-Text Alignment
--------------------------------------------------------------------------------

### Evaluation

We use SRCC (Spearman's Rank Correlation Coefficient) and PLCC (Pearson's Linear Correlation Coefficient) to evaluate the correlation between the overall alignment scores and human preferences. Additionally, we use accuracy to evaluate the consistency between the fine-grained annotations and human annotations. The final score is computed as the average of the SRCC and PLCC values, and then averaged with the accuracy score. This ensures that both the overall alignment and the fine-grained alignment are considered in the evaluation.

Their implementations are found in most of the statistics/machine learning toolboxes. For example, the demo evaluation code in Python:

import numpy as np
from scipy.optimize import curve\_fit
from scipy import stats

def logistic\_func(X, bayta1, bayta2, bayta3, bayta4):
	logisticPart = 1 + np.exp(np.negative(np.divide(X - bayta3, np.abs(bayta4))))
	yhat = bayta2 + np.divide(bayta1 - bayta2, logisticPart)
	return yhat

def fit\_function(y\_label, y\_output):
	beta = \[np.max(y\_label), np.min(y\_label), np.mean(y\_output), 0.5\]
	popt, \_ = curve\_fit(logistic\_func, y\_output, \\
		y\_label, p0=beta, maxfev=100000000)
	y\_output\_logistic = logistic\_func(y\_output, \*popt)
	
	return y\_output\_logistic

y\_output\_logistic = fit\_function(y\_label, y\_output)
PLCC = stats.pearsonr(y\_output\_logistic, y\_label)\[0\]
SRCC = stats.spearmanr(y\_output, y\_label)\[0\]

acc = np.mean(y\_ele\_label == y\_ele\_output)

score = PLCC / 4 + SRCC / 4 + acc / 2

### Baseline

We provide a baseline for the competition based on the model proposed in the paper "[EvalMuse-40K: A Reliable and Fine-Grained Benchmark with Comprehensive Human Annotations for Text-to-Image Generation Model Evaluation](https://arxiv.org/abs/2412.18150)". The source code of the baseline model is available at [https://github.com/DYEvaLab/EvalMuse](https://github.com/DYEvaLab/EvalMuse).

You can also use following algorithm as baseline (the results are also released with following username), which is accessable here:

"[TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering](http://arxiv.org/abs/2303.11897)"", code: [Github](https://github.com/Yushi-Hu/tifa)

"[Evaluating Text-to-Visual Generation with Image-to-Text Generation](https://arxiv.org/pdf/2404.01291)", code: [Github](https://github.com/linzhiqiu/t2v_metrics)

### Submission

For submitting the results, you need to follow these steps:

1.  Process the AI-generated image and record the predicted scores in a single output file named `output.json` (must be this name). The JSON file should be a list, with each item being a dictionary containing the following keys:
    
    {
    	"prompt": "attack of the pig giants, giant pigs, illustration, children's book, fictional drawing, bright colors, fantasy world, minimalist",
    	"img\_path": "SDXL-Turbo/00805.png",
    	"total\_score": null,
    	"element\_score": {
    		"pig giants (animal)": null,
    		"attack (activity)": null,
    		"giant (attribute)": null,
    		"illustration (attribute)": null,
    		"children's book (object)": null,
    		"fictional drawing (attribute)": null,
    		"bright colors (color)": null,
    		"fantasy world (location)": null,
    		"minimalist (attribute)": null
    	}
    }
    
    		
    
2.  Create a `readme.txt` (must be this name) that should contain the following lines filled in with the runtime per image (in seconds) of the solution, 1 or 0 accordingly if it employs CPU or GPU at runtime, and 1 or 0 if it employs extra data for training the models or not.Example `readme.txt` format:
    
    runtime per image \[s\] : 10.43
    CPU\[1\] / GPU\[0\] : 1
    Extra Data \[1\] / No Extra Data \[0\] : 1
    LLM\[1\] / Non-LLM\[0\] : 1
    Other description : Solution based on A+ of Timofte et al. ACCV 2014. We have a Matlab/C++ implementation, and report single core CPU runtime. The method was trained on Train 91 of Yang et al. and BSDS 200 of the Berkeley segmentation dataset.
    		
    
    The last part of the file can have any description you want about the code producing the provided results (dependencies, links, scripts, etc.).
    
3.  Create a ZIP archive named `submission.zip` (must be this name) containing only `output.json`Â and `readme.txt` and submit this zip file.

**Note:** We encourage using general-purpose LLM tuning for this challenge. However, all parameters of the model need to be localized and publicly available (such as LLAMA). Directly calling the API of closed-source models (like GPT-4) will be judged as a violation.

NTIRE Workshop and Challenges @ CVPR 2025
-----------------------------------------

Text to Image Generation Model Quality Assessment - Track 1 Image-Text Alignment
--------------------------------------------------------------------------------

These are the official rules (terms and conditions) that govern how the NTIRE challenge on Quality Assessment for AI-Generated Content - Track 1 Image-Text Alignment will operate. This challenge will be referred to as the "challenge" or the "contest" throughout the remaining part of these rules and may be named as "NTIRE" or "Quality" benchmark, challenge, or contest elsewhere (our webpage, documentation, other publications).

In these rules, "we", "our", and "us" refer to the organizers (Shuhao Han <hansh@mail.nankai.edu.cn\>, Haotian Fan <fanhaotian@bytedance.com\>, Chunle Guo <guochunle@nankai.edu.cn\> Chongyi Li <lichongyi@nankai.edu.cn\> Radu Timofte \>radu.timofte@uni-wuerzburg.de\>) of NTIRE challenge and "you" and "yourself" refer to an eligible contest participant.

Note that these official rules can change during the contest until the start of the final phase. If at any point during the contest the registered participant considers that can not anymore meet the eligibility criteria or does not agree with the changes in the official terms and conditions then it is the responsibility of the participant to send an email to the organizers such that to be removed from all the records. Once the contest is over no change is possible in the status of the registered participants and their entries.

### 1\. Contest description

This is a skill-based contest and chance plays no part in the determination of the winner (s).

The goal of the contest is to evaluate the human subjective preference of Text-to-Image generation and the challenge is called image quality assessment.

Focus of the contest: it will be made available a dataset adapted for the specific needs of the challenge. The images have a large diversity of content. We will refer to this dataset, its partition, and related materials as Evaluation Dataset. The dataset is divided into training, validation, and testing data. We focus on the perceptual content of the results, the aim is to assess the image-text alignment with the best correlation with human subjective ground truth. The participants will not have access to the ground truth images from the test data. The ranking of the participants is according to the performance of their methods on the test data. The participants will provide descriptions of their methods, details on (run)time complexity, platform, and (extra) data used for modeling. The winners will be determined according to their entries, the reproducibility of the results and uploaded codes or executables, and the above-mentioned criteria as judged by the organizers.

### 2\. Tentative contest schedule
