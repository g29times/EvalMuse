我们的情况：
项目定位：
    这是EvalMuse竞赛的参赛项目
    基于官方基线模型FGA-BLIP2进行优化
    目标是提高图文对齐评估的得分
当前阶段（Pre-dev）：
    在Windows上搭建基本环境，确保代码可以运行
    这只是准备阶段，实际训练和开发会在Linux服务器上进行
    Windows环境主要用于代码阅读和简单测试
资源获取：
    数据集：从Huggingface下载
    预训练模型：可以从两个地方下载
    Huggingface: https://huggingface.co/hanshuhao/FGA-BLIP2/resolve/main/fga_blip2.pth
    百度网盘: https://pan.baidu.com/s/1spi1E9AjQ6xNW_Wqufgm9w (密码:tgyq)
基本流程：
    数据准备 -> 环境配置 -> 模型训练 -> 评估优化
    当前阶段 ↑     后续Linux服务器阶段 ↑

项目结构和后续工作计划：
    核心代码结构：
    lavis/: 基础模型库，包含BLIP2和FGA-BLIP2的实现
    process/: 数据预处理脚本
    process_train.py: 处理标注分数和计算对齐分数方差
    element2mask.py: 将prompt中的元素映射到特定索引
    train.py: 模型训练主脚本
    eval.py: 模型评估脚本
    demo/: 演示代码
    utils.py: 通用工具函数

    竞赛评估指标：
    SRCC（排序相关系数）
    PLCC（线性相关系数）
    Accuracy（元素级预测准确率）
    最终分数 = PLCC/4 + SRCC/4 + acc/2

    工作计划：
    当前阶段（Windows）：
    [x] 项目代码分析
    [x] 环境依赖确认
    [x] 竞赛规则理解

    Linux服务器阶段：
    [x] 数据集下载和预处理
    [x] 预训练模型部署
    [x] 基线模型评估
    [x] 模型优化方案设计

在迁移到Ubuntu服务器时注意：

确保服务器Python版本为3.10（我们之前分析过依赖兼容性）
使用原始的bash脚本（scripts/download.sh）而不是我们的Windows版本
按照README.md中的步骤顺序执行：
    # 1. 下载数据集
    sh scripts/download.sh

    # 2. 数据预处理
    python3 process/process_train.py
    python3 process/element2mask.py

    # 3. 下载预训练模型到checkpoints/并训练
    # 从huggingface或百度网盘下载fga_blip2.pth放在 checkpoints 目录下
    sh scripts/train.sh

    # 4. 运行评估
    sh scripts/eval.sh

先运行基线训练
    当前配置是比较保守的：
        4个epoch
        学习率1e-5，warmup步数100
        batch size 14
        冻结视觉编码器
    这些参数适合快速得到一个基线结果
数据分析（训练同时进行）
    # 分析数据分布
    - 统计标注方差的分布
    - 分析real/synthetic提示词的比例
    - 统计不同类型元素（对象、属性等）的分布
根据基线结果决定改进方向
    如果基线效果差（PLCC/SRCC < 0.7）：
        优先调整训练参数（学习率、epoch数）
        考虑解冻部分视觉编码器层
    如果基线效果一般（0.7-0.8）：
        实现样本加权（根据标注方差）
        添加数据增强
    如果基线效果不错（>0.8）：
        专注于改进mask生成机制
        添加元素关系建模
当前准备工作检查
    [x] 数据预处理完成（train.json, train_mask.json）
    [x] 目录结构正确
    [x] 训练脚本配置正确
    [x] 预训练模型已下载到checkpoints目录

环境配置问题解决：
    1. 基础安装：
        pip install -e .  # 以开发模式安装项目
        pip install wandb  # 安装wandb用于监控训练

    2. 解决OpenCV依赖：
        # 在服务器上安装无GUI版本
        sudo apt-get update
        sudo apt-get install -y libgl1-mesa-glx
        pip install opencv-python-headless

    3. wandb 配置：
        export WANDB_API_KEY=你的API密钥
        我们将能在wandb上看到：
        训练指标：
            loss和loss_itm的变化曲线
            学习率调度曲线
            训练步数和epoch进度
        验证指标：
            PLCC（Pearson相关系数）
            SRCC（Spearman相关系数）
            Accuracy

    4. 运行训练：
        # 测试基本导入
        python3 -c "import torch; import lavis; import cv2; print('All imports successful!')"
        
        # 训练脚本参数说明
        python3 -u -m torch.distributed.run \
            --nnodes=1 \                    # 使用1个节点
            --nproc_per_node=1 \           # 每个节点1个进程（GPU数量）
            --master_port=10000 \          # 主进程端口号
            train.py \                     # 训练脚本
            --cfg-path lavis/projects/blip2/train/fga_blip2.yaml  # 配置文件
        
        # 启动训练
        sh scripts/train.sh

训练记录：
    1. 第一次训练 (2025-03-12 15:20)
        配置：
            - 学习率：1e-5
            - 批次大小：14
            - 训练轮数：4
            - 优化器：AdamW
            - 冻结视觉编码器：是
        数据集：
            - 训练集大小：32,717
            - 每轮迭代数：2,336
            - 可训练参数：186,944,543
        监控：
            - wandb项目：evalmuse-competition
            - GPU监控：功率使用正常，内存8.5GB
        训练进展：
            - 15:23 第0轮
                * 初始loss: 0.9828
                * 50步loss: 0.7440
                * 100步loss: 0.3810
                * 150步loss: 0.3341
            - 16:51 第1轮完成
                * 总耗时：44分12秒
                * 最终loss_itm：0.2502
                * 最低loss：0.1087（1950步）
                * 学习率：9e-6
            - 17:36 第2轮完成
                * 总耗时：44分15秒
                * 最终loss_itm：0.2155
                * 最低loss：0.1067（2300步）
                * 学习率：5e-6
            - 17:36 第3轮开始
                * 学习率降至：2e-6
                * 起始loss_itm：0.1922
                * 200步loss_itm：0.1879
                * 训练极其稳定

        训练效果分析：
            1. 损失函数（每轮平均loss）：
                - 第0轮：0.335
                - 第1轮：0.250（降低25.4%）
                - 第2轮：0.215（降低14.0%）
                - 第3轮：0.191（降低11.2%）
                - 总体降低：42.9%
            
            2. 学习率调度：
                - 预热期（100步）：1e-8 -> 1e-5
                - cosine衰减：1e-5 -> 1e-6
                - 衰减策略效果显著，每轮都带来性能提升
            
            3. 系统资源使用稳定：
                - GPU内存：8.5GB
                - 训练速度：1.13-1.14秒/batch
                - 每轮耗时：约44分钟
                - GPU时钟和功率使用正常

        训练结论：
            1. 模型收敛效果优秀：
                - loss持续下降，无过拟合迹象
                - 最后一轮仍有明显提升
                - 训练过程稳定，无异常波动
            
            2. 优化策略验证：
                - 冻结视觉编码器策略正确
                - cosine学习率调度效果好
                - batch_size=14合理，无OOM风险

            3. 可能的优化方向：
                - 增加训练轮数（loss仍在下降）
                - 微调学习率范围
                - 选择性解冻部分视觉编码器层

        下一步计划：
            1. 分析验证集评估结果
            2. 根据验证指标决定：
                - 是否需要增加训练轮数
                - 是否需要调整优化策略
            3. 准备完整的实验报告，包括：
                - 训练过程分析
                - 验证集性能评估
                - 改进建议


官方代码存在的问题
    数据集不一致：
    配置文件使用train_mask.json
    评估脚本使用test.json
    这可能导致训练和评估使用不同的数据
    命名混淆：
    配置文件中将test放在valid_splits中
    代码中期望val在valid_splits中
    这导致了最佳模型检查点的保存逻辑失效

数据集分析
数据集格式：
    JSON格式，包含图像提示、路径和评分信息
    每个数据项包含：
    prompt_id：提示ID
    prompt：文本提示描述
    type：类型（如"real"）
    img_path：图像路径
    total_score：总评分（训练集有值，测试集为null）
    element_score：各元素评分（如"puffin (animal)"）
数据集差异：
    train_mask.json：训练集，包含已标注的评分
    test.json：测试集，评分字段为null（需要模型预测）
文件路径问题：
    配置文件中所有数据集都指向train_mask.json
    评估脚本默认使用test.json
    这种不一致可能导致训练和评估使用不同的数据

评估流程理解
训练过程：
    使用train_mask.json作为训练数据
    由于配置问题，验证也使用train_mask.json
    由于split_name == "val"条件不满足，不会保存最佳模型
评估过程：
    使用eval.py脚本
    加载模型和测试数据集test.json
    对每个图像-文本对计算对齐分数
    计算每个元素的分数
    保存结果到JSON文件
得分计算：
    最终分数 = PLCC/4 + SRCC/4 + acc/2
    需要使用utils.py中的compute_metrics函数计算